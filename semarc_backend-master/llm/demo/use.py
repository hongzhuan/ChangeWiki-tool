import os
from openai import OpenAI
from llm.demo.config import QwenConfig


def load_prompt_template(user_input: str) -> str:
    """è¯»å–prompt.txtæ¨¡æ¿å¹¶æ›¿æ¢ç”¨æˆ·è¾“å…¥"""
    with open("prompt.txt", "r", encoding="utf-8") as f:
        template = f.read()
    return template.format(user_input=user_input)


def call_qwen(prompt: str) -> str:
    """è°ƒç”¨åƒé—®API"""
    try:
        client = OpenAI(
            api_key=QwenConfig.API_KEY,
            base_url=QwenConfig.BASE_URL
        )

        completion = client.chat.completions.create(
            model=QwenConfig.MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±è½¯ä»¶åˆ†æå·¥ç¨‹å¸ˆã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return completion.choices[0].message.content

    except Exception as e:
        return f"åƒé—®APIè°ƒç”¨å¤±è´¥: {str(e)}"


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šç”¨æˆ·æé—®
    user_question = "ç”¨Pythonå®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•"

    # ç»„åˆä¸“ä¸šæç¤ºè¯ï¼ˆä»prompt.txtåŠ è½½ï¼‰
    full_prompt = load_prompt_template(user_question)

    # è°ƒç”¨APIè·å–å›ç­”
    answer = call_qwen(full_prompt)

    print("ğŸŸ¢ ç”¨æˆ·é—®é¢˜:", user_question)
    print("ğŸ”µ AIå›ç­”:", answer)