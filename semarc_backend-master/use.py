import time
from datetime import timedelta
# from openai import OpenAI
# from config import API_KEY, proxies, API_URL_REDIRECT, LLM_MODEL, USE_PROXY, AVAIL_LLM_MODELS
from uml_to_code_generation import tools as tl
from crazy_utils_no_ui import request_gpt_model_multi_threads_with_no_ui_and_high_efficiency, \
    request_gpt_model_in_new_thread_with_no_ui, generate_manifest_and_project_folder


def call_llm(prompt: str) -> str:
    llm_kwargs = tl.get_default_kwargs()
    return request_gpt_model_in_new_thread_with_no_ui(
        inputs = prompt, 
        llm_kwargs = llm_kwargs,
        history = [],
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±è½¯ä»¶åˆ†æå·¥ç¨‹å¸ˆã€‚",
        inputs_show_user = "*")



if __name__ == "__main__":
    start_time = time.time()
    user_question = "ç”¨Pythonå®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•"
    answer = call_llm(user_question)
    print("ğŸŸ¢ ç”¨æˆ·é—®é¢˜:", user_question)
    print("ğŸ”µ AIå›ç­”:", answer)
    print(f"\nåˆ†æå®Œæˆ! æ€»è€—æ—¶: {timedelta(seconds=time.time() - start_time)} [æ—¶:åˆ†:ç§’]")