import json
import sys
import logging
import shutil
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS, cross_origin

# from llm.change_semantic_generate import AnalysisInput,SemanticChangeAnalyzer

import utils.utils
from database.auth import register_user, login_user  # â† æ–°å¢
from database.history import get_history, add_history
from database.user_profile import get_user, update_user
from architecture_change.plantuml_add_color import update_plantuml_colors
from llm.change_semantic_concurrent_generate import AnalysisInput, SemanticChangeAnalyzer
from uml_to_code_generation import tools as tl
from architecture_change.component_cluster_add_color import component_cluster_add_color
from architecture_change.replace_plantuml_color_from_graphIDfunc import \
    replace_plantuml_color_from_graphIDfunc_component_cluster
from md2json import md2json_sum, md2json
from arch_pattern_analysis import execute_parsing_and_analysis
from semantic_analysis import code_semantic_analysis, get_semantic
from cluster_project import cluster_project
from module_naming_eng import module_naming, module_naming_double_check,module_naming_dict
from merge_final_json import merge_json_files
from graph_json import graph_json
from graph_add_function_json import merge_functionality_with_clusters, convert_component_to_sum, convert_module_to_sum
from architecture_change.a2a import a2a
from architecture_change.a2a_backup import a2a_update
from architecture_change.json2rsf import json_to_rsf
from architecture_change.file_change_info import file_change
from architecture_change.architecture_change_update_json import update_json
from architecture_change.convert_json_to_plantuml import convert_json_to_plantuml
from architecture_change.get_before_after_code import get_code_diff
from gitClone.clone_repo_with_retry import clone_repo_with_retry
import os
import subprocess
import tempfile
from architecture_change.mapping_module_file_change_count import combine_method
import shutil
from architecture_change.compare_plantuml_diff import compare_plantuml_json_versions_diff
from architecture_change.mapping_module_file_change_count import combine_method_add_file_numbers_info
from flask_socketio import SocketIO, emit
import time
import threading
import architecture_change.generate_mermaid as gm
from algorithm.comparing_clusters import get_cluster_mapping
from ChangeRepo_generate import generate_architecture_change_reports
from ChangeRepo_generate import analyze_commit_log2
import json

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import stat
import architecture_change.generate_mermaid as gm
import utils.utils
from architecture_change.component_cluster_add_color import component_cluster_add_color

from database.auth import register_user, login_user  # â† æ–°å¢
from database.history import get_history, add_history
from database.user_profile import get_user, update_user

from llm.change_semantic_concurrent_generate import AnalysisInput, SemanticChangeAnalyzer
from uml_to_code_generation import tools as tl
from md2json import md2json_sum, md2json
from arch_pattern_analysis import execute_parsing_and_analysis
from semantic_analysis import code_semantic_analysis, get_semantic
from cluster_project import cluster_project
from module_naming_eng import module_naming
from merge_final_json import merge_json_files
from graph_json import graph_json
from graph_add_function_json import merge_functionality_with_clusters, convert_component_to_sum, convert_module_to_sum
from architecture_change.a2a import a2a
from architecture_change.a2a_backup import a2a_update
from architecture_change.json2rsf import json_to_rsf
from architecture_change.file_change_info import file_change
from architecture_change.architecture_change_update_json import update_json
from architecture_change.convert_json_to_plantuml import convert_json_to_plantuml
from architecture_change.merge_Graph_Entities_json_to_Whole_reverse_tree_layer import \
    merge_Graph_Entities_json_to_Whole_reverse_tree_layer
import os
import subprocess
import tempfile
from architecture_change.mapping_module_file_change_count import combine_method
import shutil
from architecture_change.compare_plantuml_diff import compare_plantuml_json_versions_diff
from architecture_change.mapping_module_file_change_count import combine_method_add_file_numbers_info
from flask_socketio import SocketIO, emit
import time
import threading
from algorithm.comparing_clusters import get_cluster_mapping
from ChangeRepo_generate import generate_architecture_change_reports
from ChangeRepo_generate import analyze_commit_log2
from pymongo import MongoClient
import os
import json

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import stat
import architecture_change.generate_mermaid as gm
import utils.utils
from architecture_change.component_cluster_add_color import component_cluster_add_color

from database.contents import \
    MAX_DOCUMENT_SIZE  # MAX_DOCUMENT_SIZE = 15*1024*1024 :contentReference[oaicite:0]{index=0}
from database.db import fs
from bson.objectid import ObjectId
from database.db import users_col, analysis_records_col, analysis_contents_col
from database.auth import register_user, login_user
from database.history import get_history, add_history, get_history_by_id
from database.user_profile import get_user, update_user
from database.contents import get_content_by_path, save_contents

from llm.change_semantic_concurrent_generate import AnalysisInput, SemanticChangeAnalyzer
from uml_to_code_generation import tools as tl
from md2json import md2json_sum, md2json
from arch_pattern_analysis import execute_parsing_and_analysis
from semantic_analysis import code_semantic_analysis, get_semantic
from cluster_project import cluster_project
from module_naming_eng import module_naming
from merge_final_json import merge_json_files
from graph_json import graph_json
from graph_add_function_json import merge_functionality_with_clusters, convert_component_to_sum, convert_module_to_sum
from architecture_change.a2a import a2a
from architecture_change.a2a_backup import a2a_update
from architecture_change.json2rsf import json_to_rsf
from architecture_change.file_change_info import file_change
from architecture_change.architecture_change_update_json import update_json
from architecture_change.convert_json_to_plantuml import convert_json_to_plantuml
import os
import subprocess
import tempfile
from architecture_change.mapping_module_file_change_count import combine_method
import shutil
from architecture_change.compare_plantuml_diff import compare_plantuml_json_versions_diff
from architecture_change.mapping_module_file_change_count import combine_method_add_file_numbers_info
from flask_socketio import SocketIO, emit
import time
import threading
from algorithm.comparing_clusters import get_cluster_mapping
from ChangeRepo_generate import generate_architecture_change_reports
from ChangeRepo_generate import analyze_commit_log2
from pymongo import MongoClient
import os

# ä»ç¯å¢ƒå˜é‡è·å– MongoDB é…ç½®ï¼ˆå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://localhost:27017')
MONGO_DB_NAME = os.getenv('MONGO_DB_NAME', 'semarc_db')

app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")  # å…è®¸è·¨åŸŸ
CORS(app, resources={r"/*": {"origins": "*"}}, supports_credentials=True,
     methods=["GET", "POST", "PUT", "DELETE"])  # å…è®¸è·¨åŸŸè¯·æ±‚

# ç¡®ä¿resultsç›®å½•å­˜åœ¨ï¼Œè·¯å¾„åŸºäºapp.pyæ–‡ä»¶æ‰€åœ¨ç›®å½•
result_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results')
os.makedirs(result_dir, exist_ok=True)
whole_project_path = ''
count = 0
rsf_version1 = ''
rsf_version2 = ''
version1_path = ''
version2_path = ''
global_repo_url = ''
clone_url_no_version = ''
version1_tag = ''
version2_tag = ''
whole_project_name = ''
local_project_name = 'SemArc_backend'
analysis_project_language = ''
repo_clone_local_path = ''
code_changes_root_path = ''
log_path = os.path.join(result_dir, 'run.log')
log_file = open(log_path, 'w', encoding='utf-8')
sys.stdout = log_file
sys.stderr = log_file
# é…ç½®æ—¥å¿—è®°å½•å™¨
# log_path = os.path.join(result_dir, 'run.log')
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s',
#     handlers=[
#         logging.FileHandler(log_path, encoding='utf-8'),  # å†™å…¥æ—¥å¿—æ–‡ä»¶
#         logging.StreamHandler()  # è¾“å‡ºåˆ°ç»ˆç«¯
#     ]
# )

# ç¤ºä¾‹ï¼šæ›¿æ¢åŸæ¥çš„ `sys.stdout` å’Œ `sys.stderr` é‡å®šå‘
logging.info("æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œæ—¥å¿—å°†åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶ã€‚")

# å…³é”®å­—æ˜ å°„ï¼šè‡ªåŠ¨è½¬æ¢ä¸ºè¶…é“¾æ¥
keyword_links = {
    "æ•°æ®åˆ†æ": "page_data_analysis.md",
    "äººå·¥æ™ºèƒ½": "page_ai.md",
    "æœºå™¨å­¦ä¹ ": "page_ml.md"
}

tasks = [
    {"id": 1, "name": "ç‰ˆæœ¬1çš„æ¶æ„é€†å‘", "progress": 0, "children": [
        {"id": 11, "name": "é™æ€ä»£ç ä¾èµ–è§£æ", "progress": 0},
        {"id": 12, "name": "ä»£ç è¯­ä¹‰ç”Ÿæˆ", "progress": 0},
        {"id": 13, "name": "æ¶æ„è¯­ä¹‰ç”Ÿæˆ", "progress": 0}
    ]},
    {"id": 2, "name": "ç‰ˆæœ¬2çš„æ¶æ„é€†å‘", "progress": 0, "children": [
        {"id": 15, "name": "é™æ€ä»£ç ä¾èµ–è§£æ", "progress": 0},
        {"id": 14, "name": "ä»£ç è¯­ä¹‰ç”Ÿæˆ", "progress": 0},
        {"id": 16, "name": "æ¶æ„è¯­ä¹‰ç”Ÿæˆ", "progress": 0}
    ]},
    {"id": 3, "name": "å˜æ›´åˆ†æ", "progress": 0, "children": [
        {"id": 17, "name": "ç»“æ„å˜æ›´åˆ†æ", "progress": 0},
        {"id": 18, "name": "æäº¤å†å²åˆ†æ", "progress": 0},
        {"id": 19, "name": "ä»£ç å®ä½“å˜æ›´", "progress": 0}
    ]},
]


def calculate_parent_progress():
    """æ›´æ–°çˆ¶ä»»åŠ¡çš„è¿›åº¦ = å­ä»»åŠ¡è¿›åº¦çš„å¹³å‡å€¼"""
    for task in tasks:
        if "children" in task:
            child_progress = [subtask["progress"] for subtask in task["children"]]
            task["progress"] = int(sum(child_progress) / len(child_progress))  # è®¡ç®—å¹³å‡è¿›åº¦


def update_progress():
    """æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œå¹¶é€æ­¥æ›´æ–°è¿›åº¦"""
    for _ in range(10):
        time.sleep(1)  # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
        for task in tasks:
            if "children" in task:
                for subtask in task["children"]:
                    subtask["progress"] = min(subtask["progress"] + 15, 100)  # å­ä»»åŠ¡å…ˆå®Œæˆ
            else:
                task["progress"] = min(task["progress"] + 10, 100)  # ç‹¬ç«‹ä»»åŠ¡

        calculate_parent_progress()  # è®¡ç®—çˆ¶ä»»åŠ¡è¿›åº¦
        socketio.emit("progress_update", {"tasks": tasks})  # å‘é€è¿›åº¦æ›´æ–°

    socketio.emit("progress_complete", {"message": "æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼"})


@app.route("/api/register", methods=["POST"])
def api_register():
    return register_user()


@app.route("/api/login", methods=["POST"])
def api_login():
    return login_user()


@app.route("/api/history/<username>", methods=["GET"])
# @app.route("/api/history", methods=["POST"])
# @cross_origin(origins="*", methods=["POST"])
def api_history(username):
    # data = request.json
    # username = data.get("username")
    return get_history(username)


@app.route("/api/history", methods=["POST"])
def api_add_history():
    return add_history()


@app.route("/api/user/<username>", methods=["GET"])
@cross_origin(origins="*", methods=["GET"])
def api_get_user(username):
    return get_user(username)


@app.route("/api/user/<username>", methods=["PUT"])
@cross_origin(origins="*", methods=["PUT"])
def api_update_user(username):
    return update_user(username)


@app.route("/api/historyContents/<record_id>", methods=["GET"])
def api_history_by_id(record_id):
    return get_history_by_id(record_id)


@app.route("/api/save_contents", methods=["POST"])
def api_save_contents():
    """
    å‰ç«¯ POST { analysisId: "<id>" }
    åç«¯æ ¹æ® analysis_records ä¸­çš„ projectName/version æ ‡ç­¾
    è°ƒç”¨ save_contents æŠŠä¸‰ä¸ªå­æ–‡ä»¶å¤¹å†™è¿› analysis_contentsã€‚
    """
    data = request.get_json(silent=True) or {}
    aid = data.get("analysisId")
    if not aid:
        return jsonify({"error": "ç¼ºå°‘ analysisId"}), 400

    rec = analysis_records_col.find_one(
        {"_id": ObjectId(aid)},
        {"projectName": 1, "version1": 1, "version2": 1}
    )
    if not rec:
        return jsonify({"error": "è®°å½•ä¸å­˜åœ¨"}), 404

    project = rec["projectName"]
    v1 = rec["version1"]
    v2 = rec["version2"]

    # è¿™é‡Œå–å…¨å±€ result_dir
    base_results = result_dir  # e.g. os.path.join(os.getcwd(), "results")

    # åªéå†è¿™ä¸‰æ¡å­ç›®å½•
    folders = [
        f"{project}-{v1}",
        f"{project}-{v2}",
        f"{project}-{v1}{v2}"
    ]
    # é’ˆå¯¹æ¯ä¸ªå­ç›®å½•ï¼Œéƒ½æŠŠå®ƒå†™å…¥ db
    for sub in folders:
        full = os.path.join(base_results, sub)
        if os.path.isdir(full):
            save_contents(aid, full)

    return jsonify({"status": "ok"}), 200


@app.route("/start")
def start_task():
    """å¯åŠ¨ä»»åŠ¡"""
    socketio.start_background_task(target=update_progress)
    return {"message": "ä»»åŠ¡å·²å¯åŠ¨"}


def process_markdown(content):
    """å°†å…³é”®å­—è½¬æ¢ä¸ºè¶…é“¾æ¥"""
    for keyword, link in keyword_links.items():
        content = content.replace(keyword, f"[{keyword}]({link})")
    return content


@app.route("/get_markdown_architecture_change_report", methods=["POST"])
def get_architecture_change_report():
    # è§£æè¯·æ±‚å‚æ•°
    data = request.get_json(silent=True) or {}
    analysis_id = data.get("analysisId")
    if not analysis_id:
        return jsonify({"error": "ç¼ºå°‘ analysisId"}), 400

    # æŸ¥ analysis_recordsï¼Œå–å‡º projectNameã€version1ã€version2
    rec = analysis_records_col.find_one(
        {"_id": ObjectId(analysis_id)},
        {"projectName": 1, "version1": 1, "version2": 1, "analysisFolder": 1}
    )
    if not rec:
        return jsonify({"error": "è®°å½•ä¸å­˜åœ¨"}), 404

    project_name = rec["projectName"]
    v1 = rec["version1"]
    v2 = rec["version2"]
    # æŒ‰çº¦å®šæ‹¼æ¥
    analysis_folder = f"{project_name}-{v1}{v2}"

    # æ‹¼å‡ºç›¸å¯¹è·¯å¾„ï¼Œä¸ save_contents å­˜å…¥çš„ Path ä¸€ä¸€å¯¹åº”
    rel_path = f"{analysis_folder}\\{project_name}_full_report.md"

    print(rel_path)
    print("å¼€å§‹ä»æ•°æ®åº“ä¸­è·å–å†…å®¹")

    # ä» MongoDB å–å†…å®¹
    content = get_content_by_path(analysis_id, rel_path)
    if content is None:
        print("Markdown æ–‡ä»¶ä¸å­˜åœ¨")
        return jsonify({"error": "Markdown æ–‡ä»¶ä¸å­˜åœ¨"}), 404

    # è¿”å›ç»™å‰ç«¯
    print("è·å–æ¶æ„å˜æ›´æŠ¥å‘ŠæˆåŠŸ")
    return jsonify({"content": content}), 200


@app.route("/get_markdown", methods=["POST"])
def get_markdown():
    data = request.get_json(silent=True) or {}
    analysis_id = data.get("analysisId")
    if not analysis_id:
        return jsonify({"error": "ç¼ºå°‘ analysisId"}), 400

    # ä»æ•°æ®åº“æŸ¥è®°å½•
    rec = analysis_records_col.find_one(
        {"_id": ObjectId(analysis_id)},
        {"projectName": 1, "version1": 1, "version2": 1, "code_changes_root_path": 1}
    )
    if not rec:
        return jsonify({"error": "è®°å½•ä¸å­˜åœ¨"}), 404

    project_name = rec["projectName"]
    v1 = rec["version1"]
    v2 = rec["version2"]
    code_changes_root = rec["code_changes_root_path"]
    code_changes_root = os.path.basename(code_changes_root)

    # æ‹¼æ¥æ–‡ä»¶è·¯å¾„åŸºæœ¬è·¯å¾„
    analysis_folder = f"{project_name}-{v1}{v2}"

    raw_label = data.get("label")
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œè¿›è¡Œ replace æ“ä½œï¼›å¦‚æœæ˜¯æ•°å­—ï¼Œç›´æ¥è½¬ä¸ºå­—ç¬¦ä¸²
    if isinstance(raw_label, str):
        label = raw_label.replace("/", "_")
    else:
        label = str(raw_label)  # æ•°å­—ç±»å‹ç›´æ¥è½¬ä¸ºå­—ç¬¦ä¸²

    category = data.get("category")
    node = data.get("node_allInfo")

    # æ‹¼æ¥ analysis_folder ç”Ÿæˆç›¸å¯¹è·¯å¾„ rel_path
    if category == 'cluster':
        rel_path = f"{analysis_folder}\\{project_name}_{label}_report.md"
    elif category == 'component':
        rel_path = f"{analysis_folder}\\{project_name}_{label}_component_report.md"
    elif category == 'file':
        file_name = label.replace("_", "----")
        rel_path = (
            f"{analysis_folder}\\code_changes\\"
            f"{code_changes_root}\\{file_name}\\semantic.txt"
        )
    elif category == 'Function':
        parent_Name = node.get("parentName", "")

        # æ‹¼æ¥ entities_changes_info_json_root çš„è·¯å¾„
        entities_changes_info_json_root = (
            f"{analysis_folder}\\code_changes\\"
            f"{code_changes_root}\\entities_changes_info.json"
        )

        before_code, after_code = get_code_diff(label, parent_Name, entities_changes_info_json_root, analysis_id)

        parent_Name1 = parent_Name.replace("/", "----")
        label_name1 = os.path.basename(node.get('entity_path', '').rstrip('\\/'))

        function_semantic_file = (
            f'{analysis_folder}\\code_changes\\'
            f'{code_changes_root}\\{parent_Name1}\\{label_name1}\\semantic.txt'
        )

        # ä» MongoDB è¯»å–å†…å®¹å¹¶è¿”å›ç»™å‰ç«¯
        content = get_content_by_path(analysis_id, function_semantic_file)
        if content is None:
            return jsonify({"error": " Function Markdown æ–‡ä»¶ä¸å­˜åœ¨"}), 404

        return jsonify({"content": content, "before_code": before_code, "after_code": after_code}), 200

    else:
        return jsonify({"error": "æœªçŸ¥çš„ category"}), 400

    # ä» MongoDB è¯»å–å†…å®¹å¹¶è¿”å›ç»™å‰ç«¯
    logging.info(f"rel_path: {rel_path}")
    logging.info(f"category: {category}")
    content = get_content_by_path(analysis_id, rel_path)
    if content is None:
        return jsonify({"error": "Markdown æ–‡ä»¶ä¸å­˜åœ¨"}), 404

    return jsonify({"content": content}), 200


@app.route("/get_markdown_architecture_version_summary_report", methods=["POST"])
@cross_origin(origins="*", methods=["POST"])
def get_architecture_version_summary_report():
    # è§£æè¯·æ±‚å‚æ•°
    data = request.get_json(silent=True) or {}
    analysis_id = data.get("analysisId")
    if not analysis_id:
        return jsonify({"error": "ç¼ºå°‘ analysisId"}), 400

    # ä» analysis_records é›†åˆä¸­æŸ¥å‡ºå¯¹åº”è®°å½•
    rec = analysis_records_col.find_one(
        {"_id": ObjectId(analysis_id)},
        {"projectName": 1, "version1": 1, "version2": 1, "analysisFolder": 1}
    )
    if not rec:
        return jsonify({"error": "è®°å½•ä¸å­˜åœ¨"}), 404

    project_name = rec["projectName"]
    v1 = rec["version1"]
    v2 = rec["version2"]
    # åŸæœ‰ analysisFolder å­—æ®µå¯èƒ½ä¿å­˜äº†ä¸å«å‰ç¼€çš„ç›®å½•åï¼Œè¿™é‡Œç›´æ¥ç”¨å¸¦ results/ å‰ç¼€çš„æ–°ç‰ˆæ‹¼æ³•
    analysis_folder = f"{project_name}-{v1}{v2}"

    # æ‹¼å‡ºä¸ save_contents æ—¶ä¸€è‡´çš„ç›¸å¯¹è·¯å¾„
    rel_path = f"{analysis_folder}\\{project_name}_conclusion.md"

    logging.info(f"rel_path: {rel_path}")

    # è°ƒç”¨ get_content_by_path ä» analysis_contents é›†åˆä¸­å–å‡º content
    content = get_content_by_path(analysis_id, rel_path)
    if content is None:
        return jsonify({"error": "Markdown æ–‡ä»¶ä¸å­˜åœ¨"}), 404

    # è¿”å›ç»™å‰ç«¯
    return jsonify({"content": content}), 200


@app.route('/get_markdown_by_key')
def get_markdown_by_key():
    key = request.args.get('key')
    # file_path = os.path.join(BASE_MD_PATH, f"{key}.md")
    file_path = "D:\\SemArc_backend\\architecture_change\\un_run.md"
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return jsonify({"content": content})
    else:
        return jsonify({"content": f"ğŸ” æœªæ‰¾åˆ°ä¸ '{key}' å¯¹åº”çš„æ–‡æ¡£"}), 404


@app.route("/api/generate_plantuml", methods=["POST"])
def get_plantuml_data():
    print("Received GET request at /api/plantuml")
    global version1_path
    global version2_path
    global version1_tag
    global version2_tag
    version1_modify_tag = version1_tag.replace("/", "")
    version2_modify_tag = version2_tag.replace("/", "")
    print("/api/plantumlä¸­version1_path:")
    print(version1_path)
    print("/api/plantumlä¸­version2_path:")
    print(version2_path)
    input_version1_json_to_plantuml = f'{version1_path}_ClusterComponent.json'
    output_path_version1_plantuml_json_data = f'{version1_path}_Plantuml_json_data.json'
    input_version2_json_to_plantuml = f'{version2_path}_ClusterComponent.json'
    output_path_version2_plantuml_json_data = f'{version2_path}_Plantuml_json_data.json'
    component2_cluster2_add_color = os.path.join(result_dir, f'{whole_project_name}-{version2_modify_tag}',
                                                 f'{whole_project_name}-{version2_modify_tag}_GraphIDFunc_modify_add_component_cluster_color.json')
    component1_cluster1_add_color = os.path.join(result_dir, f'{whole_project_name}-{version1_modify_tag}',
                                                 f'{whole_project_name}-{version1_modify_tag}_GraphIDFunc_modify_add_component_cluster_color.json')

    convert_json_to_plantuml(input_version1_json_to_plantuml, output_path_version1_plantuml_json_data)
    convert_json_to_plantuml(input_version2_json_to_plantuml, output_path_version2_plantuml_json_data)
    update_plantuml_colors(output_path_version1_plantuml_json_data, component1_cluster1_add_color,
                           output_path_version1_plantuml_json_data)
    update_plantuml_colors(output_path_version2_plantuml_json_data, component2_cluster2_add_color,
                           output_path_version2_plantuml_json_data)
    architecture_change_plantuml = compare_plantuml_json_versions_diff(output_path_version1_plantuml_json_data,
                                                                       output_path_version2_plantuml_json_data)

    plantuml_data_json = os.path.join(result_dir, f'{whole_project_name}-{version1_modify_tag}{version2_modify_tag}',
                                      f'{whole_project_name}_plantuml_data.json')
    # åœ¨plantuml_data_jsonä¸­æ·»åŠ ä¸€ä¸ªå­—æ®µï¼Œarchitecture_name: å€¼ä¸º f"{whole_project_name}æ¶æ„å˜æ›´"
    architecture_change_plantuml['architecture_name'] = f"{whole_project_name}_architecture_change"

    # ä¿®æ”¹architecture_change_plantumlä¸­componentså­—æ®µä¸‹elementsä¸­å„Itemçš„color ä½¿ç”¨component1_cluster1_add_colorå’Œcomponent2_cluster2_add_colorä¸­structureçš„itemä¸ºclusterçš„color
    # with open(component1_cluster1_add_color, 'r', encoding='utf-8') as f:
    #     component1_cluster1_add_color_data = json.load(f)
    # with open(component2_cluster2_add_color, 'r', encoding='utf-8') as f:
    #     component2_cluster2_add_color_data = json.load(f)
    # for component in architecture_change_plantuml['components']:

    with open(plantuml_data_json, 'w', encoding='utf-8') as f:
        json.dump(architecture_change_plantuml, f, ensure_ascii=False, indent=4)
    generate_mermaid = gm.json_to_mermaid(architecture_change_plantuml)
    # å°†ç”Ÿæˆçš„Mermaid jsonæ–‡æ¡£ä¿å­˜åˆ°æ–‡ä»¶
    mermaid_file_path = os.path.join(result_dir, f'{whole_project_name}-{version1_modify_tag}{version2_modify_tag}',
                                     f'{whole_project_name}_architecture_change_mermaid.mmd')
    open(mermaid_file_path, 'w', encoding='utf-8').write(generate_mermaid)
    print("generate_mermaid")
    print(generate_mermaid)
    # return {"architecture_data":architecture_change_plantuml}
    return {"mermaid_data": generate_mermaid}


@app.route("/api/get_plantuml", methods=["POST"])
def get_plantuml():
    data = request.get_json(silent=True) or {}
    analysis_id = data.get("analysisId")
    if not analysis_id:
        return {"error": "analysisId is required"}, 400

    # å…ˆè·å–ä¸€ä¸‹ project_name/version1_tag/version2_tagï¼Œç”¨æ¥æ‹¼å‡ºä¿å­˜æ—¶çš„ Path
    rec = analysis_records_col.find_one({"_id": ObjectId(analysis_id)})
    if not rec:
        logging.info("è®°å½•ä¸å­˜åœ¨")
        return {"error": "è®°å½•ä¸å­˜åœ¨"}, 404

    projectName = rec["projectName"]
    v1 = rec["version1"]
    v2 = rec["version2"]
    rel_path = f"{projectName}-{v1}{v2}\\{projectName}_architecture_change_mermaid.mmd"

    content = get_content_by_path(analysis_id, rel_path)
    if content is None:
        logging.info(f"rel_pathè·¯å¾„ï¼š{rel_path}")
        logging.info("Mermaid æ–‡ä»¶å†…å®¹æœªæ‰¾åˆ°")
        return {"error": "Mermaid æ–‡ä»¶å†…å®¹æœªæ‰¾åˆ°"}, 404

    return {"mermaid_data": content}


@app.route('/compare_architecture_change', methods=['POST'])
def architecture_change():
    global rsf_version1
    global rsf_version2
    global version1_path
    global version2_path
    global version1_tag
    global version2_tag
    global global_repo_url
    global code_changes_root_path
    print("compare_architecture_changeä¸­version1_path:")
    # eg. D:\backend\semarc_backend\results\jianshi-v1.0\jianshi-v1.0
    print(version1_path)
    print("compare_architecture_changeä¸­rsf_version1:")
    # eg. D:\backend\semarc_backend\results\jianshi-v1.0\jianshi-v1.0_rsf.rsf
    print(rsf_version1)
    print("compare_architecture_changeä¸­version2_path:")
    # eg. D:\backend\semarc_backend\results\jianshi-v2.0\jianshi-v2.0
    print(version2_path)
    print("compare_architecture_changeä¸­rsf_version2:")
    #eg. D:\backend\semarc_backend\results\jianshi-v2.0\jianshi-v2.0_rsf.rsf
    print(rsf_version2)
    version1_project_path = ''
    version2_project_path = ''
    version1_modify_tag = version1_tag.replace("/", "")
    version2_modify_tag = version2_tag.replace("/", "")
    if rsf_version1 is not '' and rsf_version2 is not '':
        version1_version2_a2a_mapping_weight_path = f'{version1_path}_{version1_modify_tag}_{version2_modify_tag}_a2a_mapping_weight.txt'
        # res, a2a_value, change_total=a2a(rsf_version1,rsf_version2)

        res, a2a_value, change_total = a2a_update(rsf_version1, rsf_version2, version1_version2_a2a_mapping_weight_path)
        # print("version1_tag",version1_tag)
        # print("version2_tag",version2_tag)
        # print("global_repo_url",global_repo_url)
        a2a_tableInfo_path = f'{version1_path}_{version1_modify_tag}_{version2_modify_tag}_a2a_mapping_weight_table.json'
        a2a_tableInfo_json = combine_method(global_repo_url, version1_tag, version2_tag,
                                            f'{version1_path}_{version1_modify_tag}_{version1_modify_tag}_file_change_count.json',
                                            rsf_version1, rsf_version2, version1_version2_a2a_mapping_weight_path,
                                            a2a_tableInfo_path)

        added, removed, moved, added_files, removed_files, moved_files, file_unit_operation_change = file_change(
            rsf_version1, rsf_version2, res, f'{version1_path}_cluster_contain_file_info.json',
            f'{version1_path}_file_unit_operation_change.json')

        a2a_tableInfo_json_add_fileInfo = combine_method_add_file_numbers_info(a2a_tableInfo_json,
                                                                               f'{version1_path}_cluster_contain_file_info.json')
        # input_version1_file = f'{version1_project_path}_GraphIDFunc.json'
        input_version1_file = f'{version1_path}_GraphIDFunc.json'
        # output_version1_file = f'{version1_project_path}_version1_change_Files.json
        output_version1_file = f'{version1_path}_GraphIDFunc_modify_add_color.json'
        component1_cluster1_add_color = f'{version1_path}_GraphIDFunc_modify_add_component_cluster_color.json'
        architecture1_change_json = update_json(input_version1_file, output_version1_file, added_files, removed_files,
                                                moved_files)
        print("architecture1_change_json")
        print(architecture1_change_json)
        input_version2_file = f'{version2_path}_GraphIDFunc.json'
        output_version2_file = f'{version2_path}_GraphIDFunc_modify_add_color.json'
        component2_cluster2_add_color = f'{version2_path}_GraphIDFunc_modify_add_component_cluster_color.json'
        architecture2_change_json = update_json(input_version2_file, output_version2_file, added_files, removed_files,
                                                moved_files)

        architecture1_component_cluster_add_color_change, architecture2_component_cluster_add_color_change = component_cluster_add_color(
            architecture1_change_json, architecture2_change_json, version1_version2_a2a_mapping_weight_path)
        with open(component1_cluster1_add_color, 'w', encoding='utf-8') as f:
            json.dump(architecture1_component_cluster_add_color_change, f, indent=4)
        with open(component2_cluster2_add_color, 'w', encoding='utf-8') as f:
            json.dump(architecture2_component_cluster_add_color_change, f, indent=4)

        # print(architecture2_change_json)
        # with open(architecture1_change_json,'r',encoding='utf-8') as f:
        #     architecture1_change_json = json.load(f)
        # with open(architecture2_change_json,'r',encoding='utf-8') as f:
        #     architecture2_change_json = json.load(f)
        # file_belongto_cluster_change = file_operation_change(rsf_version1,rsf_version2,res,added_files,removed_files,moved_files)
        # print(type(res))
        # print(res)
        version1_component_cluster_file_code_all_level = merge_Graph_Entities_json_to_Whole_reverse_tree_layer(
            os.path.join(code_changes_root_path, 'entities_changes_info.json'), component1_cluster1_add_color,
            f'{version1_path}_project_all_level_entities.json')
        version2_component_cluster_file_code_all_level = merge_Graph_Entities_json_to_Whole_reverse_tree_layer(
            os.path.join(code_changes_root_path, 'entities_changes_info.json'), component2_cluster2_add_color,
            f'{version2_path}_project_all_level_entities.json')
        print("architecture_changeä¸­version1_component_cluster_file_code_all_levelï¼š ",
              version1_component_cluster_file_code_all_level)
        print("architecture_changeä¸­version2_component_cluster_file_code_all_levelï¼š ",
              version2_component_cluster_file_code_all_level)
        res_serializable = {str(key): value for key, value in res.items()}

        # æŠŠreturnä¸­çš„å˜é‡æ›´æ–°åˆ°æ•°æ®åº“ä¸­ï¼ŒåŸï¼šåç«¯->EventBus->å‰ç«¯ï¼›ç°ï¼šåç«¯->database->å‰ç«¯
        # æ‹¿åˆ°å‰ç«¯ä¼ æ¥çš„å½“å‰åˆ†æçš„ analysisId
        data = request.get_json(silent=True) or {}
        aid = data.get("analysisId")
        if not aid:
            return jsonify({"error": "ç¼ºå°‘ analysisId"}), 400

        rec = analysis_records_col.find_one({"_id": ObjectId(aid)})
        if not rec:
            return jsonify({"error": "è®°å½•ä¸å­˜åœ¨"}), 404

        # æ›´æ–°è®°å½•ä¸­çš„å˜é‡
        raw_update_fields = {
            "a2a_value": str(a2a_value),
            "change_total": str(change_total),
            "module_weight": res_serializable,
            "added_file": added,
            "removed_file": removed,
            "moved_file": moved,
            "architecture1_change_json": version1_component_cluster_file_code_all_level,
            "architecture2_change_json": version2_component_cluster_file_code_all_level,
            "file_unit_operation_change_json": file_unit_operation_change,
            "a2a_tableInfo": a2a_tableInfo_json,
            "a2a_tableInfo_json_add_fileInfo": a2a_tableInfo_json_add_fileInfo
        }

        # è¾…åŠ©å‡½æ•°ï¼šå¯¹æ¯ä¸ªå­—æ®µï¼Œè‹¥å¤§å°è¶…é˜ˆå€¼åˆ™å­˜ GridFSï¼Œå¦åˆ™ç›´æ¥ $set
        def build_update_doc(analysis_id, fields: dict):
            set_ops = {}
            unset_ops = {}
            for key, val in fields.items():
                # è½¬æˆå­—ç¬¦ä¸²ä»¥æµ‹å¤§å°
                raw = json.dumps(val, ensure_ascii=False)
                byte_size = len(raw.encode("utf-8"))
                if byte_size > MAX_DOCUMENT_SIZE:
                    # å­˜åˆ° GridFS
                    file_id = fs.put(
                        raw.encode("utf-8"),
                        filename=f"{analysis_id}_{key}.json",
                        metadata={"analysisId": ObjectId(analysis_id), "field": key},
                        contentType="application/json"
                    )
                    # åœ¨ä¸»æ–‡æ¡£é‡Œè®°å½• gridfs_id
                    set_ops[f"{key}_gridfs_id"] = file_id
                    # å¦‚æœä¹‹å‰æœ‰ç›´æ¥å­˜å‚¨çš„å­—æ®µï¼Œç”¨ $unset æ¸…ç†
                    unset_ops[key] = ""
                else:
                    # å°æ–‡ä»¶ç›´æ¥å†™å…¥
                    set_ops[key] = val

            update_doc = {}
            if set_ops:
                update_doc["$set"] = set_ops
            if unset_ops:
                update_doc["$unset"] = unset_ops
            return update_doc

        # æ„å»ºå¹¶æ‰§è¡Œæ›´æ–°
        update_doc = build_update_doc(aid, raw_update_fields)

        analysis_records_col.update_one(
            {"_id": ObjectId(aid)},
            update_doc
        )

        return {
            "message": "Architecture change analysis completed",
            "status": "success",
            "version1": version1_tag,
            "version2": version2_tag,
            "a2a_value": str(a2a_value),
            "change_total": str(change_total),
            "module_weight": res_serializable,
            "added_file": added,
            "removed_file": removed,
            "moved_file": moved,
            # "architecture1_change_json":architecture1_component_cluster_add_color_change,
            # "architecture2_change_json": architecture2_component_cluster_add_color_change,
            "architecture1_change_json": version1_component_cluster_file_code_all_level,
            "architecture2_change_json": version2_component_cluster_file_code_all_level,
            "file_unit_operation_change_json": file_unit_operation_change,
            "a2a_tableInfo": a2a_tableInfo_json,
            "a2a_tableInfo_json_add_fileInfo": a2a_tableInfo_json_add_fileInfo
        }, 200
    else:
        return {
            "message": "è¿˜éœ€è¦é€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬æ–‡ä»¶",
            "status": "success",
        }, 200


def run_clustering_modify_right(project_url, knowledge):
    # data = request.get_json()
    global rsf_version2
    global version2_path
    global analysis_project_language
    global named_cluster_path_v1
    global cluster_component_path_v1
    project_folder = project_url
    project_name = os.path.basename(project_folder)
    # whole_project_path = project_name
    if not project_folder:
        return jsonify({"error": "project_folder is required"}), 400

    # æ„å»ºæ–‡ä»¶è·¯å¾„
    code_sem_file = os.path.join(result_dir, project_name, f'{project_name}_CodeSem.json')
    # arch_sem_file = os.path.join(result_dir, project_name, f'{project_name}_ArchSem.json')
    global arch_sem_file  #ä½¿ç”¨åŒä¸€å¥—æ¶æ„æ¨¡å¼

    # å¦‚æœæ²¡æœ‰ç”Ÿæˆå¿…è¦çš„ .json æ–‡ä»¶ï¼Œåˆ™å…ˆè°ƒç”¨ /get_semantic æ¥å£
    if not os.path.exists(code_sem_file) or not os.path.exists(arch_sem_file):
        # è°ƒç”¨ get_semantic ç”Ÿæˆæ–‡ä»¶
        # analysis_project_languageå­˜å‚¨é€†å‘åˆ†æçš„é¡¹ç›®è¯­è¨€ å€¼æ˜¯åœ¨utils.utils.get_prj_lang(clone_dir)

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ README æˆ– README.md æ–‡ä»¶
        readme_path = os.path.join(project_folder, "README")
        readme_md_path = os.path.join(project_folder, "README.md")
        if os.path.exists(readme_path):
            with open(readme_path, "r", encoding="utf-8") as readme_file:
                knowledge = readme_file.read()
        elif os.path.exists(readme_md_path):
            with open(readme_md_path, "r", encoding="utf-8") as readme_md_file:
                knowledge = readme_md_file.read()

        get_semantic(language=analysis_project_language, folder_path=project_folder, knowledge=knowledge,
                     arch_sem_path=arch_sem_file)  #é¢†åŸŸçŸ¥è¯†å‚æ•°ï¼šknowledge
        if not os.path.exists(code_sem_file) or not os.path.exists(arch_sem_file):
            return jsonify({"error": "Failed to generate semantic files"}), 500

    # è¿è¡Œèšç±»åŠŸèƒ½
    stopwords_path = 'stopwords.txt'
    # pattern_file_path = os.path.join(result_dir, project_name, f'{project_name}_ArchSem.json')
    # llm_file_path = os.path.join(result_dir, project_name, f'{project_name}_CodeSem.json')
    pattern_file_path = arch_sem_file
    llm_file_path = code_sem_file

    # åœ¨è°ƒç”¨ cluster_project å‡½æ•°ä¹‹å‰å¤„ç† project_folder
    # if os.path.exists(os.path.join(project_folder, "test")):
    #     test_folder_path = os.path.join(project_folder, "test")
    #     print(f"å‘ç° 'test' æ–‡ä»¶å¤¹ï¼Œæ­£åœ¨åˆ é™¤: {test_folder_path}")
    #     shutil.rmtree(test_folder_path)  # åˆ é™¤ 'test' æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
    #     print("'test' æ–‡ä»¶å¤¹å·²åˆ é™¤")

    # try:
    # è°ƒç”¨ cluster_project å‡½æ•°æ‰§è¡Œèšç±»æ“ä½œ
    cluster_project(
        data_paths=[project_folder],
        gt_json_paths=None,
        resolution=1.2,
        result_folder_name=None,
        cache_dir='./cache',
        save_to_csvfile=True,
        stopword_files=[stopwords_path],
        generate_figures=True,
        pattern_file=[pattern_file_path],
        llm_file=[llm_file_path]
    )

    # def remove_empty_clusters(cluster_result_path):
    # # åŠ è½½ cluster_result.json
    #     with open(cluster_result_path, 'r', encoding='utf-8') as f:
    #         cluster_result = json.load(f)

    #     new_cluster={
    #         "@schemaVersion":"1.0",
    #         "name": "clustering",
    #         "structure": []
    #     }
    #     # æ‰¾åˆ°ç©ºç°‡çš„ç¼–å·
    #     empty_clusters = [
    #         int(group["name"]) for group in cluster_result["structure"]
    #         if group["@type"] == "group" and not group["nested"]
    #     ]

    #     print(f"Empty clusters found: {empty_clusters}")  # è°ƒè¯•ä¿¡æ¯

    #     # åŠ è½½ cluster_result_named.json
    #     # with open(named_cluster_path, 'r', encoding='utf-8') as f:
    #     #     named_clusters = json.load(f)

    #     # è¿‡æ»¤æ‰ç©ºç°‡
    #     new_cluster["structure"] = [
    #         module for module in cluster_result["structure"]
    #         if int(module["name"]) not in empty_clusters
    #     ]

    #     # ä¿å­˜æ›´æ–°åçš„ cluster_result_named.json
    #     with open(cluster_result_path, 'w', encoding='utf-8') as f:
    #         json.dump(new_cluster, f, indent=4, ensure_ascii=False)

    #     print("Empty clusters removed from named clusters.")

    # # æ–°å¢æ­¥éª¤ï¼šç§»é™¤ç©ºç°‡
    # remove_empty_clusters(
    #     os.path.join(result_dir, project_name, f'cluster_result.json'),
    # )

    #æ¨¡å—å‘½å
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”å¼€å§‹æ¨¡å—å‘½åâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    module_names=os.path.join(result_dir, project_name, f'cluster_result_named.json')
    # named_cluster_path_v2,cluster_component_path_v2=module_naming(result_dir,project_name,os.path.join(result_dir, project_name, f'cluster_result.json'), llm_file_path,knowledge=knowledge)  #é¢†åŸŸçŸ¥è¯†åŠ åœ¨è¿™é‡Œ
    # ç›®å½•åšæ¨¡å—
    named_cluster_path_v2,cluster_component_path_v2=module_naming_dict(result_dir,project_name,os.path.join(result_dir, project_name, f'cluster_result_pkg.json'), llm_file_path,knowledge=knowledge)  #é¢†åŸŸçŸ¥è¯†åŠ åœ¨è¿™é‡Œ

    # ç»Ÿä¸€ä¸¤ä¸ªç‰ˆæœ¬çš„æ¨¡å—å‘½å
    # module_naming_double_check(named_cluster_path_v1, cluster_component_path_v1,named_cluster_path_v2, cluster_component_path_v2)
    # print("**********æ¨¡å—å‘½åæ£€æŸ¥å®Œæˆï¼**********\n")

    #åˆå¹¶ç»„ä»¶-æ¨¡å—å’Œæ¨¡å—-æ–‡ä»¶json
    final_json_path = os.path.join(result_dir, project_name, f"{project_name}_Final.json")
    if not os.path.exists(final_json_path):
        merge_json_files(cluster_component_path_v2, named_cluster_path_v2, final_json_path)

    #ç”Ÿæˆæ–°çš„graph id
    graph_id_path = os.path.join(result_dir, project_name, f'{project_name}_GraphID.json')
    if not os.path.exists(graph_id_path):
        graph_json(final_json_path, graph_id_path)

    #æ·»åŠ functionality
    component_sum_path = os.path.join(result_dir, project_name, f'{project_name}_ComponentSum.json')
    if not os.path.exists(component_sum_path):
        convert_component_to_sum(pattern_file_path, component_sum_path)
    # æ·»åŠ ç”Ÿæˆ ModuleSum.json çš„é€»è¾‘
    module_sum_path = os.path.join(result_dir, project_name, f'{project_name}_ModuleSum.json')
    if not os.path.exists(module_sum_path):
        convert_module_to_sum(module_names, module_sum_path)

    graph_id_func_path = os.path.join(result_dir, project_name, f'{project_name}_GraphIDFunc.json')  # å¯è§†åŒ–jsonæ–‡ä»¶

    if not os.path.exists(graph_id_func_path):
        merge_functionality_with_clusters(graph_id_path, llm_file_path, graph_id_func_path, module_sum_path)
        merge_functionality_with_clusters(graph_id_func_path, component_sum_path, graph_id_func_path, module_sum_path)

    json_file_path = os.path.join(os.path.join(result_dir, project_name, f'{project_name}_GraphIDFunc.json'))
    rsf_version2 = json_to_rsf(
        os.path.join(os.path.join(result_dir, project_name, f'{project_name}_NamedClusters.json')),
        os.path.join(os.path.join(result_dir, project_name, f'{project_name}_rsf.rsf')))
    version2_path = os.path.join(os.path.join(result_dir, project_name, f'{project_name}'))
    print("run_clustering_modify_rightä¸­version2_path", version2_path)
    if os.path.exists(json_file_path):
        print("json_file_path")
        print(json_file_path)
        print("project_name")
        print(project_name)
        # è¯»å–JSONæ–‡ä»¶å†…å®¹
        with open(json_file_path, 'r', encoding='utf-8') as json_file:
            file_content = json.load(json_file)
        with open(json_file_path, 'r', encoding='utf-8') as f:
            reverse_layer_graph = json.load(f)
            # print(reverse_layer_graph)
        # è¿”å›JSONæ–‡ä»¶å†…å®¹ä½œä¸ºå“åº”
        return {
            "sharedFile": file_content,
            "reverse_layer_graph": reverse_layer_graph
        }
    else:
        return {"error": "File not found"}


def run_clustering_modify(project_url, domain_knowledge=""):
    # data = request.get_json()
    global rsf_version1
    global version1_path
    global analysis_project_language
    project_folder = project_url
    print("run_clustering_project_folder", project_folder)
    project_name = os.path.basename(project_folder)
    # whole_project_path = project_name
    if not project_folder:
        return jsonify({"error": "project_folder is required"}), 400

    # å°†æ¶æ„æ¨¡å¼å’Œç»„ä»¶ä½œä¸ºå…¨å±€å˜é‡
    global arch_sem_file
    # æ„å»ºæ–‡ä»¶è·¯å¾„
    code_sem_file = os.path.join(result_dir, project_name, f'{project_name}_CodeSem.json')
    arch_sem_file = os.path.join(result_dir, project_name, f'{project_name}_ArchSem.json')

    # å¦‚æœæ²¡æœ‰ç”Ÿæˆå¿…è¦çš„ .json æ–‡ä»¶ï¼Œåˆ™å…ˆè°ƒç”¨ /get_semantic æ¥å£
    if not os.path.exists(code_sem_file) or not os.path.exists(arch_sem_file):
        # è°ƒç”¨ get_semantic ç”Ÿæˆæ–‡ä»¶
        # project_lang= utils.utils.get_prj_lang()
        # analysis_project_language å­˜å‚¨é€†å‘åˆ†æçš„é¡¹ç›®è¯­è¨€

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ README æˆ– README.md æ–‡ä»¶
        readme_path = os.path.join(project_folder, "README")
        readme_md_path = os.path.join(project_folder, "README.md")
        if os.path.exists(readme_path):
            with open(readme_path, "r", encoding="utf-8") as readme_file:
                domain_knowledge = readme_file.read()
        elif os.path.exists(readme_md_path):
            with open(readme_md_path, "r", encoding="utf-8") as readme_md_file:
                domain_knowledge = readme_md_file.read()

        get_semantic(language=analysis_project_language, folder_path=project_folder, knowledge=domain_knowledge,
                     arch_sem_path=arch_sem_file)  #é¢†åŸŸçŸ¥è¯†åŠ åœ¨è¿™é‡Œ
        if not os.path.exists(code_sem_file) or not os.path.exists(arch_sem_file):
            return jsonify({"error": "Failed to generate semantic files"}), 500

    # è¿è¡Œèšç±»åŠŸèƒ½
    stopwords_path = 'stopwords.txt'
    pattern_file_path = os.path.join(result_dir, project_name, f'{project_name}_ArchSem.json')
    llm_file_path = os.path.join(result_dir, project_name, f'{project_name}_CodeSem.json')

    # åœ¨è°ƒç”¨ cluster_project å‡½æ•°ä¹‹å‰å¤„ç† project_folder
    # if os.path.exists(os.path.join(project_folder, "test")):
    # test_folder_path = os.path.join(project_folder, "test")
    # print(f"å‘ç° 'test' æ–‡ä»¶å¤¹ï¼Œæ­£åœ¨åˆ é™¤: {test_folder_path}")
    # shutil.rmtree(test_folder_path)  # åˆ é™¤ 'test' æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
    # print("'test' æ–‡ä»¶å¤¹å·²åˆ é™¤")
    # try:
    # è°ƒç”¨ cluster_project å‡½æ•°æ‰§è¡Œèšç±»æ“ä½œ
    cluster_project(
        data_paths=[project_folder],
        gt_json_paths=None,
        resolution=1.2,
        result_folder_name=None,
        cache_dir='./cache',
        save_to_csvfile=True,
        stopword_files=[stopwords_path],
        generate_figures=True,
        pattern_file=[pattern_file_path],
        llm_file=[llm_file_path]
    )

    def remove_empty_clusters(cluster_result_path):
        # åŠ è½½ cluster_result.json
        with open(cluster_result_path, 'r', encoding='utf-8') as f:
            cluster_result = json.load(f)

        new_cluster = {
            "@schemaVersion": "1.0",
            "name": "clustering",
            "structure": []
        }
        # æ‰¾åˆ°ç©ºç°‡çš„ç¼–å·
        empty_clusters = [
            int(group["name"]) for group in cluster_result["structure"]
            if group["@type"] == "group" and not group["nested"]
        ]

        print(f"Empty clusters found: {empty_clusters}")  # è°ƒè¯•ä¿¡æ¯

        # åŠ è½½ cluster_result_named.json
        # with open(named_cluster_path, 'r', encoding='utf-8') as f:
        #     named_clusters = json.load(f)

        # è¿‡æ»¤æ‰ç©ºç°‡
        new_cluster["structure"] = [
            module for module in cluster_result["structure"]
            if int(module["name"]) not in empty_clusters
        ]

        # ä¿å­˜æ›´æ–°åçš„ cluster_result_named.json
        with open(cluster_result_path, 'w', encoding='utf-8') as f:
            json.dump(new_cluster, f, indent=4, ensure_ascii=False)

        print("Empty clusters removed from named clusters.")

    # æ–°å¢æ­¥éª¤ï¼šç§»é™¤ç©ºç°‡
    remove_empty_clusters(
        os.path.join(result_dir, project_name, f'cluster_result.json'),
    )

    #æ¨¡å—å‘½å
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”å¼€å§‹æ¨¡å—å‘½åâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    module_names=os.path.join(result_dir, project_name, f'cluster_result_named.json')
    global named_cluster_path_v1
    global cluster_component_path_v1
    # named_cluster_path_v1,cluster_component_path_v1=module_naming(result_dir,project_name,os.path.join(result_dir, project_name, f'cluster_result.json'), llm_file_path,knowledge=domain_knowledge)  #é¢†åŸŸçŸ¥è¯†åŠ åœ¨è¿™é‡Œ
    # ç›®å½•ç»“æ„åšæ¨¡å—ä¸éœ€è¦æ¨¡å—å‘½å
    module_names=os.path.join(result_dir, project_name, f'cluster_result_named.json')
    global named_cluster_path_v1
    global cluster_component_path_v1
    named_cluster_path_v1,cluster_component_path_v1=module_naming_dict(result_dir,project_name,os.path.join(result_dir, project_name, f'cluster_result_pkg.json'), llm_file_path,knowledge=domain_knowledge)  #é¢†åŸŸçŸ¥è¯†åŠ åœ¨è¿™é‡Œ

    #åˆå¹¶ç»„ä»¶-æ¨¡å—å’Œæ¨¡å—-æ–‡ä»¶json
    final_json_path = os.path.join(result_dir, project_name, f"{project_name}_Final.json")
    if not os.path.exists(final_json_path):
        merge_json_files(cluster_component_path_v1, named_cluster_path_v1, final_json_path)

    #ç”Ÿæˆæ–°çš„graph id
    graph_id_path = os.path.join(result_dir, project_name, f'{project_name}_GraphID.json')
    if not os.path.exists(graph_id_path):
        graph_json(final_json_path, graph_id_path)

    #æ·»åŠ functionality
    component_sum_path = os.path.join(result_dir, project_name, f'{project_name}_ComponentSum.json')
    if not os.path.exists(component_sum_path):
        convert_component_to_sum(pattern_file_path, component_sum_path)
    # æ·»åŠ ç”Ÿæˆ ModuleSum.json çš„é€»è¾‘
    module_sum_path = os.path.join(result_dir, project_name, f'{project_name}_ModuleSum.json')
    if not os.path.exists(module_sum_path):
        convert_module_to_sum(module_names, module_sum_path)

    graph_id_func_path = os.path.join(result_dir, project_name, f'{project_name}_GraphIDFunc.json')  # å¯è§†åŒ–jsonæ–‡ä»¶

    if not os.path.exists(graph_id_func_path):
        merge_functionality_with_clusters(graph_id_path, llm_file_path, graph_id_func_path, module_sum_path)
        merge_functionality_with_clusters(graph_id_func_path, component_sum_path, graph_id_func_path, module_sum_path)

    json_file_path = os.path.join(os.path.join(result_dir, project_name, f'{project_name}_GraphIDFunc.json'))
    rsf_version1 = json_to_rsf(
        os.path.join(os.path.join(result_dir, project_name, f'{project_name}_NamedClusters.json')),
        os.path.join(os.path.join(result_dir, project_name, f'{project_name}_rsf.rsf')))
    version1_path = os.path.join(os.path.join(result_dir, project_name, f'{project_name}'))
    print("run_clustering_modifyä¸­version1_path", version1_path)
    if os.path.exists(json_file_path):
        print("json_file_path")
        print(json_file_path)
        print("project_name")
        print(project_name)
        # è¯»å–JSONæ–‡ä»¶å†…å®¹
        with open(json_file_path, 'r', encoding='utf-8') as json_file:
            file_content = json.load(json_file)
        with open(json_file_path, 'r', encoding='utf-8') as f:
            reverse_layer_graph = json.load(f)
            # print(reverse_layer_graph)
        # è¿”å›JSONæ–‡ä»¶å†…å®¹ä½œä¸ºå“åº”
        return {
            "sharedFile": file_content,
            "reverse_layer_graph": reverse_layer_graph
        }
    else:
        return {"error": "File not found"}


@app.route("/get_git_refs", methods=["POST"])
def get_git_refs():
    """è·å– Git ä»“åº“çš„åˆ†æ”¯å’Œ Tag åˆ—è¡¨"""
    global global_repo_url
    global repo_clone_local_path
    global clone_url_no_version
    data = request.json
    repo_url = data.get("repo_url")
    global_repo_url = repo_url
    project_name = repo_url.split('/')[-1].replace('.git', '')
    print(project_name)
    clone_dir = os.path.abspath(project_name)
    clone_url_no_version = clone_dir
    repo_clone_local_path = clone_dir
    print("clone_dir:")
    print(clone_dir)
    if not repo_url:
        return jsonify({"error": "ç¼ºå°‘ä»“åº“ URL"}), 400

    # åˆ¤æ–­æ˜¯å¦ä¸º Chromium
    if "chromium" in repo_url:
        depot_tools_dir = os.path.abspath("depot_tools")
        if not os.path.exists(depot_tools_dir):
            subprocess.run([
                "git", "clone", "https://chromium.googlesource.com/chromium/tools/depot_tools.git", depot_tools_dir
            ], check=True)
        env = os.environ.copy()
        env["PATH"] = depot_tools_dir + env["PATH"]
        chromium_dir = os.path.abspath("chromium")
        repo_clone_local_path = chromium_dir
        if not os.path.exists(chromium_dir):
            subprocess.run(["fetch", "chromium"], cwd=os.path.dirname(chromium_dir), check=True)
        # è·å–åˆ†æ”¯å’Œ tag
        result = subprocess.run(['git', '-C', chromium_dir, 'ls-remote', '--refs'], capture_output=True, text=True,
                                check=True)
    else:
        project_name = repo_url.split('/')[-1].replace('.git', '')
        clone_dir = os.path.abspath(project_name)
        repo_clone_local_path = clone_dir
        if os.path.exists(clone_dir) and len(os.listdir(clone_dir)) > 5:
            subprocess.run(["git", "-C", clone_dir, "fetch", "--all"], check=True)
        else:
            subprocess.run(["git", "clone", "--bare", "--depth", "1", repo_url, clone_dir], check=True)
        result = subprocess.run(['git', '-C', clone_dir, 'ls-remote', '--refs'], capture_output=True, text=True,
                                check=True)
    # if not repo_url:
    #     return jsonify({"error": "ç¼ºå°‘ä»“åº“ URL"}), 400
    #
    # try:
    #     if os.path.exists(clone_dir) and len(os.listdir(clone_dir)) > 5:
    #         # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œè¿›å…¥ç›®å½•å¹¶æ›´æ–°
    #         print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œå°è¯•æ›´æ–°ä»“åº“...")
    #         subprocess.run(["git", "-C", clone_dir, "fetch", "--all"], check=True, stdout=subprocess.PIPE,
    #                        stderr=subprocess.PIPE)
    #     elif os.path.exists(clone_dir) and len(os.listdir(clone_dir)) <= 5:
    #         print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œä½†å†…å®¹è¿‡å°‘ï¼Œé‡æ–°å…‹éš†ã€‚")
    #         shutil.rmtree(clone_dir, ignore_errors=True)  # ä½¿ç”¨ shutil.rmtree åˆ é™¤ç›®å½•
    #         subprocess.run(["git", "clone", "--bare", repo_url, clone_dir], check=True, stdout=subprocess.PIPE,
    #                        stderr=subprocess.PIPE)
    #         print(f"ä»“åº“å·²æˆåŠŸå…‹éš†åˆ° {clone_dir}")
    #     else:
    #         # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå…‹éš†ä»“åº“
    #         print(f"æ­£åœ¨å…‹éš†ä»“åº“ {repo_url} åˆ° {clone_dir}...")
    #         subprocess.run(["git", "clone", "--bare", repo_url, clone_dir], check=True, stdout=subprocess.PIPE,
    #                        stderr=subprocess.PIPE)

    # è·å–åˆ†æ”¯å’Œæ ‡ç­¾
    result = subprocess.run(['git', '-C', clone_dir, 'ls-remote', '--refs'], capture_output=True, text=True,
                            check=True)
    branches = []
    tags = []
    for line in result.stdout.split("\n"):
        if not line.strip():
            continue
        ref_hash, ref_name = line.split("\t")
        if ref_name.startswith("refs/heads/"):
            branches.append(ref_name.replace("refs/heads/", ""))
        elif ref_name.startswith("refs/tags/"):
            tags.append(ref_name.replace("refs/tags/", ""))

    return jsonify({"branches": branches, "tags": tags})

    # except subprocess.CalledProcessError as e:
    #     return jsonify({"error": f"æ— æ³•è·å–ä»“åº“ä¿¡æ¯: {e.stderr.decode('utf-8')}"}), 500


@app.route("/select_version", methods=["POST"])
def select_version():
    global version1_tag
    global whole_project_name
    global analysis_project_language
    run_clustering_modify_json = {}
    """æ¥æ”¶ç”¨æˆ·é€‰æ‹©çš„ Git ç‰ˆæœ¬"""
    data = request.json
    repo_url = data.get("repo_url")
    selected_version = data.get("selected_version")
    domain_knowledge = data.get("domain_knowledge", "")
    version1_tag = selected_version

    if not repo_url or not selected_version:
        return jsonify({"error": "ç¼ºå°‘ä»“åº“ URL æˆ–ç‰ˆæœ¬"}), 400

    try:
        # è·å–é¡¹ç›®åç§°
        project_name = repo_url.split('/')[-1].replace('.git', '')
        whole_project_name = project_name
        # åœ¨æ ¹ç›®å½•ä¸‹åˆ›å»ºå­ç›®å½•ï¼Œå‘½åä¸º "é¡¹ç›®åç§°-ç‰ˆæœ¬å·"
        clone_dir_version = selected_version.replace("/", "")
        clone_dir = os.path.abspath(f"{project_name}-{clone_dir_version}")
        print(f"å…‹éš†ç›®å½•: {clone_dir}")

        # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†
        if os.path.exists(clone_dir) and len(os.listdir(clone_dir)) > 5:
            print(os.listdir(clone_dir))
            print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†ã€‚")
        elif os.path.exists(clone_dir) and len(os.listdir(clone_dir)) <= 5:
            #åˆ é™¤clone_dirç„¶åå…‹éš†
            print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œä½†å†…å®¹è¿‡å°‘ï¼Œé‡æ–°å…‹éš†ã€‚")
            # subprocess.run(["rm", "-rf", clone_dir], check=True, text=True) # åˆ é™¤ç›®å½•
            #windowsä¸‹åˆ é™¤ç›®å½•
            shutil.rmtree(clone_dir)  # åˆ é™¤ç›®å½•  
            # subprocess.run(["git", "clone", "--branch", selected_version, "--depth", "1", repo_url, clone_dir], check=True, text=True)
            clone_repo_with_retry(repo_url=repo_url, selected_version=selected_version, clone_dir=clone_dir)

        else:
            # ä½¿ç”¨ --branch å…‹éš†æŒ‡å®šç‰ˆæœ¬åˆ°å­ç›®å½•
            # print(f"æ­£åœ¨å…‹éš†ä»“åº“ {repo_url} çš„ {selected_version} ç‰ˆæœ¬åˆ° {clone_dir}...")
            # subprocess.run(
            #     ["git", "clone", "--branch", selected_version, "--depth", "1", repo_url, clone_dir],
            #     check=True,
            #     text=True
            # )
            # print(f"ä»“åº“å·²æˆåŠŸå…‹éš†åˆ° {clone_dir}")
            clone_repo_with_retry(repo_url=repo_url, selected_version=selected_version, clone_dir=clone_dir)
        # éªŒè¯æ ‡ç­¾æ˜¯å¦å­˜åœ¨
        # def tag_exists(tag, repo_dir):
        #     result = subprocess.run(
        #         ["git", "-C", repo_dir, "rev-parse", "--verify", tag],
        #         stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        #     )
        #     return result.returncode == 0

        # if not tag_exists(selected_version, clone_dir):
        #     return jsonify({"error": f"Tag {selected_version} ä¸å­˜åœ¨"}), 400

        # æ‹‰å–å®Œæ•´å†å²
        if os.path.exists(os.path.join(clone_dir, ".git", "shallow")):
            subprocess.run(["git", "-C", clone_dir, "fetch", "--unshallow"], check=True, text=True)
        else:
            subprocess.run(["git", "-C", clone_dir, "fetch", "--all"], check=True, text=True)

        analysis_project_language = utils.utils.get_prj_lang(clone_dir)
        # è°ƒç”¨åç»­å¤„ç†é€»è¾‘
        run_clustering_modify_json = run_clustering_modify(clone_dir, domain_knowledge)
        # print("ç‰ˆæœ¬ä¸€ï¼šrun_clustering_modify_json", run_clustering_modify_json)

    except subprocess.CalledProcessError as e:
        print(f"Git å…‹éš†å¤±è´¥: {e}")
        return jsonify({"error": f"Git å…‹éš†å¤±è´¥: {e}"}), 500

    return {
        "message": "ç‰ˆæœ¬1é€†å‘å®Œæˆ",
        "repo_url": repo_url,
        "selected_version": selected_version,
        "run_clustering_modify_json": run_clustering_modify_json.get('reverse_layer_graph', {})
    }, 200


@app.route("/select_version_right", methods=["POST"])
def select_version_right():
    global version1_tag
    global version2_tag
    global repo_clone_local_path
    global clone_url_no_version
    """æ¥æ”¶ç”¨æˆ·é€‰æ‹©çš„ Git ç‰ˆæœ¬"""
    data = request.json
    repo_url = data.get("repo_url")
    selected_version = data.get("selected_version")
    domain_knowledge = data.get("domain_knowledge", "")
    version2_tag = selected_version
    project_name = repo_url.split('/')[-1].replace('.git', '')
    project_result_dir = os.path.abspath(os.path.join("results"))
    print("version2_tag_select_version", version2_tag)
    os.makedirs(os.path.join(os.path.join("results"), f"{project_name}-{version1_tag}{version2_tag}"),
                exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    commit_log_dir = os.path.abspath(
        os.path.join("results", f'{project_name}-{version1_tag}{version2_tag}', f"{project_name}_log.txt"))
    log_json_dir = os.path.abspath(
        os.path.join("results", f'{project_name}-{version1_tag}{version2_tag}', f"{project_name}_log.json"))
    commit_log_module = os.path.abspath(
        os.path.join("results", f'{project_name}-{version1_tag}{version2_tag}', f"{project_name}_log_module.json"))
    recovered_result = os.path.abspath(os.path.join("results"))
    if not repo_url or not selected_version:
        return jsonify({"error": "ç¼ºå°‘ä»“åº“ URL æˆ–ç‰ˆæœ¬"}), 400

    try:
        # è·å–é¡¹ç›®åç§°
        project_name = repo_url.split('/')[-1].replace('.git', '')
        # åœ¨æ ¹ç›®å½•ä¸‹åˆ›å»ºå­ç›®å½•ï¼Œå‘½åä¸º "é¡¹ç›®åç§°-ç‰ˆæœ¬å·"
        clone_dir_version = selected_version.replace("/", "")
        clone_dir = os.path.abspath(f"{project_name}-{clone_dir_version}")
        # clone_dir = os.path.abspath(f"{project_name}-{selected_version}")
        log_dir = commit_log_dir

        print(f"å…‹éš†ç›®å½•: {clone_dir}")
        print(f"æ—¥å¿—ç›®å½•: {log_dir}")

        # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†
        if os.path.exists(clone_dir) and len(os.listdir(clone_dir)) > 5:
            print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œè·³è¿‡å…‹éš†ã€‚")
        elif os.path.exists(clone_dir) and len(os.listdir(clone_dir)) <= 5:
            #åˆ é™¤clone_dirç„¶åå…‹éš†
            # print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œä½†å†…å®¹è¿‡å°‘ï¼Œé‡æ–°å…‹éš†ã€‚")
            # subprocess.run(["rm", "-rf", clone_dir], check=True, text=True) # åˆ é™¤ç›®å½•  
            # clone_repo_with_retry(repo_url=repo_url,selected_version=selected_version,clone_dir=clone_dir)
            print(f"ç›®å½• {clone_dir} å·²å­˜åœ¨ï¼Œä½†å†…å®¹è¿‡å°‘ï¼Œé‡æ–°å…‹éš†ã€‚")
            shutil.rmtree(clone_dir, ignore_errors=True)  # ä½¿ç”¨ shutil.rmtree åˆ é™¤ç›®å½•
            clone_repo_with_retry(repo_url=repo_url, selected_version=selected_version, clone_dir=clone_dir)
        else:
            # ä½¿ç”¨ --branch å…‹éš†æŒ‡å®šç‰ˆæœ¬åˆ°å­ç›®å½•
            print(f"æ­£åœ¨å…‹éš†ä»“åº“ {repo_url} çš„ {selected_version} ç‰ˆæœ¬åˆ° {clone_dir}...")
            # subprocess.run(
            #     ["git", "clone", "--branch", selected_version, "--depth", "1", repo_url, clone_dir],
            #     check=True,
            #     text=True
            # )
            # print(f"ä»“åº“å·²æˆåŠŸå…‹éš†åˆ° {clone_dir}")
            clone_repo_with_retry(repo_url=repo_url, selected_version=selected_version, clone_dir=clone_dir)

        # è°ƒç”¨åç»­å¤„ç†é€»è¾‘
        run_clustering_right_modify_json = run_clustering_modify_right(clone_dir, domain_knowledge)
        print("ç‰ˆæœ¬äºŒï¼šrun_clustering_right_modify_json", run_clustering_right_modify_json)

    except subprocess.CalledProcessError as e:
        print(f"Git å…‹éš†å¤±è´¥: {e}")
        return jsonify({"error": f"Git å…‹éš†å¤±è´¥: {e}"}), 500

    # éªŒè¯æ ‡ç­¾æ˜¯å¦å­˜åœ¨
    # def tag_exists(tag, repo_dir):
    #     result = subprocess.run(
    #         ["git", "-C", repo_dir, "rev-parse", "--verify", tag],
    #         stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
    #     )
    #     return result.returncode == 0

    # if not tag_exists(version1_tag, clone_dir):
    #     return jsonify({"error": f"Tag {version1_tag} ä¸å­˜åœ¨"}), 400
    # if not tag_exists(version2_tag, clone_dir):
    #     return jsonify({"error": f"Tag {version2_tag} ä¸å­˜åœ¨"}), 400

    # # æ‹‰å–å®Œæ•´å†å²
    # if os.path.exists(os.path.join(clone_dir, ".git", "shallow")):
    #     subprocess.run(["git", "-C", clone_dir, "fetch", "--unshallow"], check=True, text=True)
    # else:
    #     subprocess.run(["git", "-C", clone_dir, "fetch", "--all"], check=True, text=True)

    # # æå–æäº¤æ—¥å¿—
    # with open(log_dir, "w", encoding="utf-8") as f:
    #     subprocess.run(
    #         [
    #             "git", "-C", clone_dir, "log", f"{version1_tag}..{version2_tag}",
    #             "--pretty=format:commit %H%n%ad, %s%n",
    #             "--date=format:%Y.%m.%d",
    #             "--numstat"
    #         ],
    #         stdout=f,
    #         check=True,
    #         text=True
    #     )
    #     print(f"commit log å·²æˆåŠŸä¿å­˜åˆ° {log_dir}")

    # # è°ƒç”¨åç»­å¤„ç†é€»è¾‘
    # analyze_commit_log2(commit_log_dir, log_json_dir)
    # generate_architecture_change_reports(project_name, version1_tag, version2_tag, log_json_dir, commit_log_module, recovered_result)

    # Step 2. æå–æäº¤æ—¥å¿—
    try:
        # è¿›å…¥å…‹éš†åçš„ä»“åº“ç›®å½•
        original_dir = os.getcwd()
        # os.chdir(clone_dir)
        os.chdir(clone_url_no_version)
        print(f"åˆ‡æ¢åˆ°ä»“åº“ç›®å½•: {os.getcwd()}")

        # æ£€æŸ¥ tag æ˜¯å¦å­˜åœ¨
        # def tag_exists(tag):
        #     result = subprocess.run(
        #         ["git", "rev-parse", "--verify", tag],
        #         stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        #     )
        #     return result.returncode == 0

        # if not tag_exists(version1_tag):
        #     print(f"Tag {version1_tag} ä¸å­˜åœ¨")
        #     raise Exception(f"Tag {version1_tag} ä¸å­˜åœ¨")

        # if not tag_exists(version2_tag):
        #     print(f"Tag {version2_tag} ä¸å­˜åœ¨")
        #     raise Exception(f"Tag {version2_tag} ä¸å­˜åœ¨")

        # æ‹‰å–å®Œæ•´å†å²
        # if os.path.exists(".git/shallow"):
        #     subprocess.run(["git", "fetch", "--unshallow"], check=True, text=True)
        # else:
        #     subprocess.run(["git", "fetch"], check=True, text=True)

        # æå– commit log
        with open(log_dir, "w", encoding="utf-8") as f:
            try:
                subprocess.run(
                    [
                        "git", "log", f"{version1_tag}..{version2_tag}",
                        "--pretty=format:commit %H%n%ad, %s%n",
                        "--date=format:%Y.%m.%d",
                        "--numstat",
                        "--"
                    ],
                    stdout=f,
                    check=True,
                    text=True
                )
                print(f"commit log å·²æˆåŠŸä¿å­˜åˆ° {log_dir}")
            except subprocess.CalledProcessError as e:
                print(f"git log æ‰§è¡Œå¤±è´¥: {e}")
                f.write(f"\ngit log æ‰§è¡Œå¤±è´¥: {e}\n")
                raise

        check = os.path.join(project_result_dir, f'{project_name}-{version1_tag}{version2_tag}',
                             f"{project_name}_full_report.md")
        print(f'æ–‡ä»¶å¤¹:{check}')
        if os.path.exists(check):
            print(f"æ–‡ä»¶å¤¹å­˜åœ¨: {check}")
        else:
            analyze_commit_log2(commit_log_dir, log_json_dir)
            # generate_architecture_change_reports(project_name, version1_tag, version2_tag, log_json_dir,
            #                                      commit_log_module, recovered_result)
    finally:
        os.chdir(original_dir)
        print(f"è¿”å›åˆ°åŸç›®å½•: {original_dir}")

    return {
        "message": "ç‰ˆæœ¬2é€†å‘å®Œæˆ",
        "repo_url": repo_url,
        "selected_version": selected_version,
        "run_clustering_right_modify_json": run_clustering_right_modify_json.get('reverse_layer_graph', {})
    }, 200


@app.route('/generate_code_changes', methods=['POST'])
def generate_code_changes():
    global global_repo_url
    global version1_tag
    global version2_tag
    global whole_project_name
    global repo_clone_local_path
    global code_changes_root_path
    print("generate_code_changesä¸­çš„ global_repo_url", global_repo_url)
    version1_modify_tag = version1_tag.replace("/", "")
    version2_modify_tag = version2_tag.replace("/", "")
    code_change_store_path = os.path.join(result_dir,
                                          f'{whole_project_name}-{version1_modify_tag}{version2_modify_tag}',
                                          "code_changes")
    code_analysis_input = AnalysisInput(global_repo_url, version1_tag, version2_tag, code_change_store_path,
                                        repo_clone_local_path)
    analyzer = SemanticChangeAnalyzer(code_analysis_input)
    analyzer.run_analysis()  # ä¸å†è¿”å›å®ä½“æ•°å€¼
    # è·å– changes_root_path
    # changes_root_path = analyzer.changes_root_path
    code_changes_root_path = analyzer.changes_root_path
    print("code_changesä¸­çš„changes_root_path ", code_changes_root_path)
    # print("select_version_rightä¸­çš„changes_root_path ", changes_root_path) :D:\backend\semarc_backend\results\libuv-v1.44.2v1.48.0\code_changes/libuv-0c1fa696aa502eb749c2c4735005f41ba00a27b8-e9f29cb984231524e3931aa0ae2c5dae1a32884e
    # print("num_entities ",num_entities)

    # æŠŠ code_changes_root_path å­˜å‚¨åˆ°æ•°æ®åº“ä¸­
    data = request.get_json(silent=True) or {}
    aid = data.get('analysisId')
    if not aid:
        return jsonify({"error": "ç¼ºå°‘ analysisId"}), 400

    rec = analysis_records_col.find_one({'_id': ObjectId(aid)})
    if not rec:
        return jsonify({"error": "è®°å½•ä¸å­˜åœ¨"}), 404

    # æ›´æ–°è®°å½•ä¸­çš„å˜é‡
    update_fields = {
        'code_changes_root_path': code_changes_root_path,
        'repo_clone_local_path': repo_clone_local_path
    }
    analysis_records_col.update_one(
        {"_id": ObjectId(aid)},
        {"$set": update_fields}
    )

    #ä»£ç å˜æ›´åˆ†æåè°ƒç”¨æŠ¥å‘Šç”Ÿæˆ
    log_json_dir = os.path.join(result_dir, f'{whole_project_name}-{version1_modify_tag}{version2_modify_tag}',
                                f"{whole_project_name}_log.json")
    commit_log_module = os.path.join(result_dir, f'{whole_project_name}-{version1_modify_tag}{version2_modify_tag}',
                                     f"{whole_project_name}_log_module.json")
    generate_architecture_change_reports(whole_project_name, version1_tag, version2_tag, log_json_dir,
                                         commit_log_module, result_dir, repo_clone_local_path)

    return {
        "message": "ä»£ç å˜æ›´åˆ†æå®Œæˆ",
        "status": "success",
    }


if __name__ == "__main__":
    app.run(host="0.0.0.0",
            port=8000, debug=True)
